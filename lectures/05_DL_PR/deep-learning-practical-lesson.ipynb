{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tuw-nlp-ie/tuw-nlp-ie-2022WS/blob/main/lectures/05_DL_PR/deep-learning-practical-lesson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUJ9cPl-Fry3"
   },
   "source": [
    "# Natural Language Processing and Information Extraction\n",
    "## Deep learning - practical session\n",
    "\n",
    "__Nov 11, 2022__\n",
    "\n",
    "__Ádám Kovács__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-08G4DavFry5"
   },
   "source": [
    "During this lecture we are going to use a classification dataset from a shared task: SemEval 2019 - Task 6. \n",
    "The dataset is about Identifying and Categorizing Offensive Language in Social Media.\n",
    "__Preparation:__\n",
    "- You will need the Semeval dataset (we will have code to download it)\n",
    "- You will need to install pytorch:\n",
    "    - pip install torch \n",
    "- You will also need to have pandas, torchtext, numpy and scikit learn installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_7oXPa0Fry6"
   },
   "source": [
    "We are going to use an open source library for building optimized deep learning models that can be run on GPUs, the library is called [Pytorch](https://pytorch.org/docs/stable/index.html). It is one of the most widely used libraries for building neural networks/deep learning models.\n",
    "\n",
    "In this lecture we are mostly using pure PyTorch models, but there are multiple libraries available to make it even easier to build neural networks. You are free to use them in your projects.\n",
    "Just to name a few:\n",
    "- TorchText: https://pytorch.org/text/stable/index.html\n",
    "- AllenNLP: https://github.com/allenai/allennlp\n",
    "\n",
    "__NOTE: It is advised to use Google Colab for this laboratory for free access to GPUs, and also for reproducibility.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZt9zm2LFry6",
    "outputId": "4e75b0b5-a7a2-4f59-fdf0-3b7f2238fd30"
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4Xkf04_Fry7"
   },
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDFv7TnVFry8"
   },
   "source": [
    "## Download the dataset and load it into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0gNCsikFry8",
    "outputId": "5b322ec3-2674-4fec-9507-b271c51d9c7d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "u = urllib.request.URLopener()\n",
    "u.retrieve(\n",
    "    \"https://raw.githubusercontent.com/ZeyadZanaty/offenseval/master/datasets/training-v1/offenseval-training-v1.tsv\",\n",
    "    \"data/offenseval.tsv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xxkoCVUFry8"
   },
   "source": [
    "## Read in the dataset into a Pandas DataFrame\n",
    "Use `pd.read_csv` with the correct parameters to read in the dataset. If done correctly, `DataFrame` should have 5 columns, \n",
    "`id`, `tweet`, `subtask_a`, `subtask_b`, `subtask_c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14AzL6GHFry9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eef320fdacfdf485",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    train_data = pd.read_csv(\"./data/offenseval.tsv\", sep=\"\\t\")\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "rNCxGm0LFry-",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8f39b3b86623648c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a4337459-acbe-4602-8a3c-6ec8a79592f2"
   },
   "outputs": [],
   "source": [
    "train_data_unprocessed = read_dataset()\n",
    "\n",
    "train_data_unprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8sKz1zsFry-"
   },
   "source": [
    "## Convert `subtask_a` into a binary label\n",
    "The task is to classify the given tweets into two category: _offensive(OFF)_ , _not offensive (NOT)_. For machine learning algorithms you will need integer labels instead of strings. Add a new column to the dataframe called `label`, and transform the `subtask_a` column into a binary integer label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQ4VazgwFry-",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-595b437c85da4194",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def transform(train_data):\n",
    "    labels = {\"NOT\": 0, \"OFF\": 1}\n",
    "\n",
    "    train_data[\"label\"] = [labels[item] for item in train_data.subtask_a]\n",
    "    train_data[\"tweet\"] = train_data[\"tweet\"].str.replace(\"@USER\", \"\")\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSHYynQ3Fry_",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-56fa86b834804581",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_data = transform(train_data_unprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1vVLQwfFrzA"
   },
   "source": [
    "## Train a simple neural network on this dataset\n",
    "\n",
    "In this notebook we are going to build different neural architectures on the task:\n",
    "- A simple one layered feed forward neural network (FNN) with one-hot encoded vectors\n",
    "- Adding more layers to the FNN, making it a deep neural network\n",
    "- Instead of using one-hot encoded vectors we are going to add embedding vectors to the architecture, that takes the sequential nature of natural texts into account\n",
    "- Then we will train LSTM networks\n",
    "- At last, we will also build a Transformer architecture, that currently achieves SOTA results on a lot of tasks\n",
    "\n",
    "First we will build one-hot-encoded vectors for each sentence, and then use a simple feed forward neural network to predict the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWIAidfzFrzA",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c242629b22cb523",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First we need to import pytorch and set a fixed random seed number for reproducibility\n",
    "import torch\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypaOR-1EFrzA"
   },
   "source": [
    "### Split the dataset into a train and a validation dataset\n",
    "Use the random seed for splitting. You should split the dataset into 70% training data and 30% validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSeRGX4KFrzB",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-20ba609174c640e7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "\n",
    "def split_data(train_data, random_seed):\n",
    "    tr_data, val_data = split(train_data, test_size=0.3, random_state=SEED)\n",
    "    return tr_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bK6m0rRFrzB",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0e8a125310d3fea9",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tr_data, val_data = split_data(train_data, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHxBtAYNFrzB"
   },
   "source": [
    "### Use CountVectorizer to prepare the features for the sentences\n",
    "_CountVectorizer_ is a great tool from _sklearn_ that helps us with basic preprocessing steps. It has lots of parameters to play with, you can check the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). It will:\n",
    "- Tokenize, lowercase the text\n",
    "- Filter out stopwords\n",
    "- Convert the text into one-hot encoded vectors\n",
    "- Select the _n_-best features\n",
    "\n",
    "We fit CountVectorizer using _3000_ features\n",
    "\n",
    "We will also _lemmatize_ texts using the _nltk_ package and its lemmatizer. Check the [docs](https://www.nltk.org/_modules/nltk/stem/wordnet.html) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shzT5AX0FrzC",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0943811065a971f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "dc5d747f-2c28-47fe-bf92-2fd3e6667a5f"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "\n",
    "def prepare_vectorizer(tr_data):\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=3000, tokenizer=LemmaTokenizer(), stop_words=\"english\"\n",
    "    )\n",
    "\n",
    "    word_to_ix = vectorizer.fit(tr_data.tweet)\n",
    "\n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xq4NYTdcFrzC",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a2c6658aef3041dc",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "2bfc0491-9249-44b1-db40-215f0f46c23e"
   },
   "outputs": [],
   "source": [
    "word_to_ix = prepare_vectorizer(tr_data)\n",
    "# The vocab size is the length of the vocabulary, or the length of the feature vectors\n",
    "VOCAB_SIZE = len(word_to_ix.vocabulary_)\n",
    "assert VOCAB_SIZE == 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer can directly transform any sentence into a one-hot encoded vector based on the corpus it was built upon.\n",
    "\n",
    "![onehot](https://miro.medium.com/max/886/1*_da_YknoUuryRheNS-SYWQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix.transform([\"Hello my name is adam\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4SD37O4FrzE",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d1819598c663eb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the correct device\n",
    "# It is important that every array should be on the same device or the training won't work\n",
    "# A device could be either the cpu or the gpu if it is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrlPM_d1FrzC"
   },
   "source": [
    "### Prepare the DataLoader for batch processing\n",
    "\n",
    "The __prepare_dataloader(..)__ function will take the training and the validation dataset and convert them to one-hot encoded vectors with the help of the initialized CountVectorizer.\n",
    "\n",
    "We prepare two FloatTensors and LongTensors for the converted tweets and labels of the training and the validation data.\n",
    "\n",
    "Then zip together the vectors with the labels as a list of tuples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-6VMlD-FrzD",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67b120b4ea6ba288",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Preparing the data loaders for the training and the validation sets\n",
    "# PyTorch operates on it's own datatype which is very similar to numpy's arrays\n",
    "# They are called Torch Tensors: https://pytorch.org/docs/stable/tensors.html\n",
    "# They are optimized for training neural networks\n",
    "def prepare_dataloader(tr_data, val_data, word_to_ix):\n",
    "    # First we transform the tweets into one-hot encoded vectors\n",
    "    # Then we create Torch Tensors from the list of the vectors\n",
    "    # It is also inportant to send the Tensors to the correct device\n",
    "    # All of the tensors should be on the same device when training\n",
    "    tr_data_vecs = torch.FloatTensor(word_to_ix.transform(tr_data.tweet).toarray()).to(\n",
    "        device\n",
    "    )\n",
    "    tr_labels = torch.LongTensor(tr_data.label.tolist()).to(device)\n",
    "\n",
    "    val_data_vecs = torch.FloatTensor(\n",
    "        word_to_ix.transform(val_data.tweet).toarray()\n",
    "    ).to(device)\n",
    "    val_labels = torch.LongTensor(val_data.label.tolist()).to(device)\n",
    "\n",
    "    tr_data_loader = [(sample, label) for sample, label in zip(tr_data_vecs, tr_labels)]\n",
    "    val_data_loader = [\n",
    "        (sample, label) for sample, label in zip(val_data_vecs, val_labels)\n",
    "    ]\n",
    "\n",
    "    return tr_data_loader, val_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQG5rdlpFrzD",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-212fb18e207761c4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tr_data_loader, val_data_loader = prepare_dataloader(tr_data, val_data, word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxHwnckHFrzD"
   },
   "source": [
    "- __We have the correct lists now, it is time to initialize the DataLoader objects!__\n",
    "- __Create two DataLoader objects with the lists we have created__\n",
    "- __Shuffle the training data but not the validation data!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1DPygZyFrzE"
   },
   "outputs": [],
   "source": [
    "# We then define a BATCH_SIZE for our model\n",
    "# Usually we don't feed the whole dataset into our model at once\n",
    "# For this we have the BATCH_SIZE parameter\n",
    "# Try to experiment with different sized batches and see if changing this will improve the performance or not!\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BT3hbbGjFrzD",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96ac025a45bc4fec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# The DataLoader(https://pytorch.org/docs/stable/data.html) class helps us to prepare the training batches\n",
    "# It has a lot of useful parameters, one of it is _shuffle_ which will randomize the training dataset in each epoch\n",
    "# This can also improve the performance of our model\n",
    "def create_dataloader_iterators(tr_data_loader, val_data_loader, BATCH_SIZE):\n",
    "    train_iterator = DataLoader(\n",
    "        tr_data_loader,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    valid_iterator = DataLoader(\n",
    "        val_data_loader,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_iterator, valid_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zvvqcuk3FrzE",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7b88321ec3ee1096",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator = create_dataloader_iterators(\n",
    "    tr_data_loader, val_data_loader, BATCH_SIZE\n",
    ")\n",
    "assert type(train_iterator) == torch.utils.data.dataloader.DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WUTDwv2FrzE"
   },
   "source": [
    "### Building the first PyTorch model\n",
    "At first, the model will contain a single Linear layer that takes one-hot-encoded vectors and trainsforms it into the dimension of the __NUM_LABELS__(how many classes we are trying to predict). Then, run through the output on a softmax activation to produce probabilites of the classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqwUwPUdFrzF",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00fb572132edf99a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Note that we could add more Linear Layers here connected to each other\n",
    "        # Then we would also need to have a HIDDEN_SIZE hyperparameter as an input to our model\n",
    "        # Then, with activation functions between them (e.g. RELU) we could have a \"Deep\" model\n",
    "        # This is just an example for a shallow network\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec, sequence_lens):\n",
    "        # Ignore sequence_lens for now!\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        # Softmax will provide a probability distribution among the classes\n",
    "        # We can then use this for our loss function\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lr-4sjO3FrzF",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3cbec9b993598632",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The INPUT_DIM is the size of our input vectors\n",
    "INPUT_DIM = VOCAB_SIZE\n",
    "# We have only 2 classes\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVxRlyR-FrzF",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-203697b21306ccb9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Init the model\n",
    "# At first it is untrained, the weights are assigned random\n",
    "model = BoWClassifier(OUTPUT_DIM, INPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEIXHRUTFrzF"
   },
   "outputs": [],
   "source": [
    "# Set the optimizer and the loss function!\n",
    "# https://pytorch.org/docs/stable/optim.html\n",
    "import torch.optim as optim\n",
    "\n",
    "# The optimizer will update the weights of our model based on the loss function\n",
    "# This is essential for correct training\n",
    "# The _lr_ parameter is the learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0pDsTdKFrzG",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9852177c09074615",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Copy the model and the loss function to the correct device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BSC-ttlFrzG",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-50c925cbe0576fd3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert model.linear.out_features == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCrTcch_FrzG"
   },
   "source": [
    "### Training and evaluating PyTorch models\n",
    "- __calculate_performance__: This should calculate the batch-wise precision, recall, and fscore of your model!\n",
    "- __train__ - Train your model on the training data! This function should set the model to training mode, then use the given iterator to iterate through the training samples and make predictions using the provided model. You should then propagate back the error with the loss function and the optimizer. Finally return the average epoch loss and performance!\n",
    "- __evaluate__ - Evaluate your model on the validation dataset. This function is essentially the same as the trainnig function, but you should set your model to eval mode and don't propagate back the errors to your weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TWQekCdFrzG",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1818f7b4bce37196",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def calculate_performance(preds, y):\n",
    "    \"\"\"\n",
    "    Returns precision, recall, fscore per batch\n",
    "    \"\"\"\n",
    "    # Get the predicted label from the probabilities\n",
    "    rounded_preds = preds.argmax(1)\n",
    "\n",
    "    # Calculate the correct predictions batch-wise and calculate precision, recall, and fscore\n",
    "    # WARNING: Tensors here could be on the GPU, so make sure to copy everything to CPU\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "        rounded_preds.cpu(), y.cpu()\n",
    "    )\n",
    "\n",
    "    return precision[1], recall[1], fscore[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhzACM5IFrzG",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea8beb3df906f9a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    # We will calculate loss and accuracy epoch-wise based on average batch accuracy\n",
    "    epoch_loss = 0\n",
    "    epoch_prec = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_fscore = 0\n",
    "\n",
    "    # You always need to set your model to training mode\n",
    "    # If you don't set your model to training mode the error won't propagate back to the weights\n",
    "    model.train()\n",
    "\n",
    "    # We calculate the error on batches so the iterator will return matrices with shape [BATCH_SIZE, VOCAB_SIZE]\n",
    "    for batch in iterator:\n",
    "        text_vecs = batch[0]\n",
    "        labels = batch[1]\n",
    "        sen_lens = []\n",
    "        texts = []\n",
    "\n",
    "        # This is for later!\n",
    "        if len(batch) > 2:\n",
    "            sen_lens = batch[2]\n",
    "            texts = batch[3]\n",
    "\n",
    "        # We reset the gradients from the last step, so the loss will be calculated correctly (and not added together)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # This runs the forward function on your model (you don't need to call it directly)\n",
    "        predictions = model(text_vecs, sen_lens)\n",
    "\n",
    "        # Calculate the loss and the accuracy on the predictions (the predictions are log probabilities, remember!)\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        prec, recall, fscore = calculate_performance(predictions, labels)\n",
    "\n",
    "        # Propagate the error back on the model (this means changing the initial weights in your model)\n",
    "        # Calculate gradients on parameters that requries grad\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # We add batch-wise loss to the epoch-wise loss\n",
    "        epoch_loss += loss.item()\n",
    "        # We also do the same with the scores\n",
    "        epoch_prec += prec.item()\n",
    "        epoch_recall += recall.item()\n",
    "        epoch_fscore += fscore.item()\n",
    "    return (\n",
    "        epoch_loss / len(iterator),\n",
    "        epoch_prec / len(iterator),\n",
    "        epoch_recall / len(iterator),\n",
    "        epoch_fscore / len(iterator),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASFglaVtFrzH",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-810fadb1db8e2028",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The evaluation is done on the validation dataset\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_prec = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_fscore = 0\n",
    "    # On the validation dataset we don't want training so we need to set the model on evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Also tell Pytorch to not propagate any error backwards in the model or calculate gradients\n",
    "    # This is needed when you only want to make predictions and use your model in inference mode!\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # The remaining part is the same with the difference of not using the optimizer to backpropagation\n",
    "        for batch in iterator:\n",
    "            text_vecs = batch[0]\n",
    "            labels = batch[1]\n",
    "            sen_lens = []\n",
    "            texts = []\n",
    "\n",
    "            if len(batch) > 2:\n",
    "                sen_lens = batch[2]\n",
    "                texts = batch[3]\n",
    "\n",
    "            predictions = model(text_vecs, sen_lens)\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            prec, recall, fscore = calculate_performance(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_prec += prec.item()\n",
    "            epoch_recall += recall.item()\n",
    "            epoch_fscore += fscore.item()\n",
    "\n",
    "    # Return averaged loss on the whole epoch!\n",
    "    return (\n",
    "        epoch_loss / len(iterator),\n",
    "        epoch_prec / len(iterator),\n",
    "        epoch_recall / len(iterator),\n",
    "        epoch_fscore / len(iterator),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzRshE3-FrzH",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73c8635f8fc4a7fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# This is just for measuring training time!\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBcx1vxBFrzH"
   },
   "source": [
    "### Training loop!\n",
    "Below is the training loop of our model! Try to set an EPOCH number that will correctly train your model :) (it is not underfitted but neither overfitted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGaJR3xTFrzH"
   },
   "outputs": [],
   "source": [
    "def training_loop(epoch_number=15):\n",
    "    # Set an EPOCH number!\n",
    "    N_EPOCHS = epoch_number\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "\n",
    "    # We loop forward on the epoch number\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model on the training set using the dataloader\n",
    "        train_loss, train_prec, train_rec, train_fscore = train(\n",
    "            model, train_iterator, optimizer, criterion\n",
    "        )\n",
    "        # And validate your model on the validation set\n",
    "        valid_loss, valid_prec, valid_rec, valid_fscore = evaluate(\n",
    "            model, valid_iterator, criterion\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        # If we find a better model, we save the weights so later we may want to reload it\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"tut1-model.pt\")\n",
    "\n",
    "        print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print(\n",
    "            f\"\\tTrain Loss: {train_loss:.3f} | Train Prec: {train_prec*100:.2f}% | Train Rec: {train_rec*100:.2f}% | Train Fscore: {train_fscore*100:.2f}%\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\t Val. Loss: {valid_loss:.3f} |  Val Prec: {valid_prec*100:.2f}% | Val Rec: {valid_rec*100:.2f}% | Val Fscore: {valid_fscore*100:.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIm70jWgQq21",
    "outputId": "e88508b0-dd40-48b7-9d56-0b9066d23eca"
   },
   "outputs": [],
   "source": [
    "training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4B_wdUgFrzI"
   },
   "source": [
    "\n",
    "__NOTE: DON'T FORGET TO RERUN THE MODEL INITIALIZATION WHEN YOU ARE TRYING TO RUN THE MODEL MULTIPLE TIMES. IF YOU DON'T REINITIALIZE THE MODEL IT WILL CONTINUE THE TRAINING WHERE IT HAS STOPPED LAST TIME AND DOESN'T RUN FROM SRATCH!__\n",
    "\n",
    "These lines:\n",
    "\n",
    "```python\n",
    "model = BoWClassifier(OUTPUT_DIM, INPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "```\n",
    "\n",
    "This will reinitialize the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOpuzSyfPwBh"
   },
   "outputs": [],
   "source": [
    "def reinitialize(model):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.NLLLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgflJA47P3O-"
   },
   "outputs": [],
   "source": [
    "reinitialize(BoWClassifier(OUTPUT_DIM, INPUT_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFcqasArFrzI"
   },
   "source": [
    "## Add more linear layers to the model and experiment with other hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSv9C_heFrzJ"
   },
   "source": [
    "### More layers\n",
    "\n",
    "Currently we only have a single linear layers in our model. We are now adding more linear layers to the model.\n",
    "We also introduce a HIDDEN_SIZE parameter that will be the size of the intermediate representation between the linear layers. Also adding a RELU activation function between the linear layers.\n",
    "\n",
    "See more:\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "- https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_S4Ytz8HFrzJ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f71eea2d6e70ad97",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BoWDeepClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size, hidden_size):\n",
    "        super(BoWDeepClassifier, self).__init__()\n",
    "        # First linear layer\n",
    "        self.linear1 = nn.Linear(vocab_size, hidden_size)\n",
    "        # Non-linear activation function between them\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # Second layer\n",
    "        self.linear2 = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec, sequence_lens):\n",
    "        # Run the input vector through every layer\n",
    "        output = self.linear1(bow_vec)\n",
    "        output = self.relu(output)\n",
    "        output = self.linear2(output)\n",
    "\n",
    "        # Get the probabilities\n",
    "        return F.log_softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5tw97UGFrzK"
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 200\n",
    "learning_rate = 0.001\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7flTZ3vgFrzK"
   },
   "outputs": [],
   "source": [
    "model = BoWDeepClassifier(OUTPUT_DIM, INPUT_DIM, HIDDEN_SIZE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNZV21ShFrzK",
    "outputId": "4bc2112f-1c98-47c7-f8cd-a58bbc662df1"
   },
   "outputs": [],
   "source": [
    "training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRInQ1ToFrzK"
   },
   "source": [
    "## Implement automatic early-stopping in the training loop\n",
    "Early stopping is a very easy method to avoid the overfitting of your model.\n",
    "\n",
    "We could:\n",
    "- Save the training and the validation loss of the last two epochs (if you are atleast in the third epoch)\n",
    "- If the loss increased in the last two epoch on the training data but descreased or stagnated in the validation data, you should stop the training automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pblBEo87FrzL",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7d460cc4aa3dd437",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# REINITIALIZE YOUR MODEL TO GET A CORRECT RUN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAN4Y5s3FrzL"
   },
   "source": [
    "## Handling class imbalance\n",
    "Our data is imbalanced, the first class has twice the population of the second class.\n",
    "\n",
    "One way of handling imbalanced data is to weight the loss function, so it penalizes errors on the smaller class.\n",
    "\n",
    "Look at the documentation of the loss function: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "\n",
    "Set the weights based on the inverse population of the classes (so the less sample a class has, more the errors will be penalized!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9_wXYqlFrzL",
    "outputId": "f5caf253-e85b-4e70-f9c8-b4434a4b0e3f"
   },
   "outputs": [],
   "source": [
    "tr_data.groupby(\"label\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FoqDTQIFrzL",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6ebf131781a3332d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "weights = torch.Tensor([1, 2])\n",
    "criterion = nn.NLLLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBu230DWRYwK"
   },
   "source": [
    "## Adding an Embedding Layer to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We only used one-hot-encoded vectors as our features until now\n",
    "- Now we will introduce an [embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer into our network.\n",
    "- We will feed the words into our network one-by-one, and the layer will learn a dense vector representation for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![embeddingbag](https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png)\n",
    "\n",
    "_from pytorch.org_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5uYW1OmUpfJ"
   },
   "outputs": [],
   "source": [
    "# Get the analyzer to get the word-id mapping from CountVectorizer\n",
    "an = word_to_ix.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Syg7NPVxUsWZ",
    "outputId": "e29a5029-6cef-4699-b403-ba237a0fbd84"
   },
   "outputs": [],
   "source": [
    "an(\"hello my name is adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jRJ_cbeeUvMy",
    "outputId": "5e6ea91c-52d9-47eb-b609-948b0a5d934f"
   },
   "outputs": [],
   "source": [
    "max(word_to_ix.vocabulary_, key=word_to_ix.vocabulary_.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DG9xZYBQ7VNQ",
    "outputId": "1956ab04-bd4f-426f-e053-4c663bacfec4"
   },
   "outputs": [],
   "source": [
    "len(word_to_ix.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbU3fq40UxyR"
   },
   "outputs": [],
   "source": [
    "def create_input(dataset, analyzer, vocabulary):\n",
    "    dataset_as_indices = []\n",
    "\n",
    "    # We go through each tweet in the dataset\n",
    "    # We need to add two additional symbols to the vocabulary\n",
    "    # We have 3000 features, ranged 0-2999\n",
    "    # We add 3000 as an id for the \"unknown\" words not among the features\n",
    "    # 3001 will be the symbol for padding, but about this later!\n",
    "    for tweet in dataset:\n",
    "        tokens = analyzer(tweet)\n",
    "        token_ids = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # if the token is in the vocab, we add the id\n",
    "            if token in vocabulary:\n",
    "                token_ids.append(vocabulary[token])\n",
    "            # else we add the id of the unknown token\n",
    "            else:\n",
    "                token_ids.append(3000)\n",
    "\n",
    "        # if we removed every token during preprocessing (stopword removal, lemmatization), we add the unknown token to the list so it won't be empty\n",
    "        if not token_ids:\n",
    "            token_ids.append(3000)\n",
    "        dataset_as_indices.append(torch.LongTensor(token_ids).to(device))\n",
    "\n",
    "    return dataset_as_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDZmvZmwU3oI"
   },
   "outputs": [],
   "source": [
    "# We add the length of the tweets so sentences with similar lengths will be next to each other\n",
    "# This can be important because of padding\n",
    "tr_data[\"length\"] = tr_data.tweet.str.len()\n",
    "val_data[\"length\"] = val_data.tweet.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3SypHXUVAOR",
    "outputId": "83d0630a-da0f-45cc-adc3-68b61d812ceb"
   },
   "outputs": [],
   "source": [
    "tr_data.tweet.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJyVrIpLVHjA"
   },
   "outputs": [],
   "source": [
    "tr_data = tr_data.sort_values(by=\"length\")\n",
    "val_data = val_data.sort_values(by=\"length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLxMeEF6VJto"
   },
   "outputs": [],
   "source": [
    "# We create the dataset as ids of tokens\n",
    "dataset_as_ids = create_input(tr_data.tweet, an, word_to_ix.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXRm0Qc0VMRo",
    "outputId": "cd490df2-1a6c-4410-96dc-c5a6bffff656"
   },
   "outputs": [],
   "source": [
    "dataset_as_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "- We didn't need to take care of input padding when using one-hot-encoded vectors\n",
    "- Padding handles different sized inputs\n",
    "- We can pad sequences from the left, or from the right\n",
    "\n",
    "![padding](https://miro.medium.com/max/1218/1*zsIXWoN0_CE9PXzmY3tIjQ.png)\n",
    "\n",
    "_image from https://towardsdatascience.com/nlp-preparing-text-for-deep-learning-model-using-tensorflow2-461428138657_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlFl5MTtVb8J"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# pad_sequence will take care of the padding\n",
    "# we will need to provide a padding_value to it\n",
    "padded = pad_sequence(dataset_as_ids, batch_first=True, padding_value=3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCPFCrMbXIAs"
   },
   "outputs": [],
   "source": [
    "def prepare_dataloader_with_padding(tr_data, val_data, word_to_ix):\n",
    "    # First create the id representations of the input vectors\n",
    "    # Then pad the sequences so all of the input is the same size\n",
    "    # We padded texts for the whole dataset, this could have been done batch-wise also!\n",
    "    tr_data_vecs = pad_sequence(\n",
    "        create_input(tr_data.tweet, an, word_to_ix.vocabulary_),\n",
    "        batch_first=True,\n",
    "        padding_value=3001,\n",
    "    )\n",
    "    tr_labels = torch.LongTensor(tr_data.label.tolist()).to(device)\n",
    "    tr_lens = torch.LongTensor(\n",
    "        [len(i) for i in create_input(tr_data.tweet, an, word_to_ix.vocabulary_)]\n",
    "    )\n",
    "\n",
    "    # We also add the texts to the batches\n",
    "    # This is for the Transformer models, you wont need this in the next experiments\n",
    "    tr_sents = tr_data.tweet.tolist()\n",
    "\n",
    "    val_data_vecs = pad_sequence(\n",
    "        create_input(val_data.tweet, an, word_to_ix.vocabulary_),\n",
    "        batch_first=True,\n",
    "        padding_value=3001,\n",
    "    )\n",
    "    val_labels = torch.LongTensor(val_data.label.tolist()).to(device)\n",
    "    val_lens = torch.LongTensor(\n",
    "        [len(i) for i in create_input(val_data.tweet, an, word_to_ix.vocabulary_)]\n",
    "    )\n",
    "\n",
    "    val_sents = val_data.tweet.tolist()\n",
    "\n",
    "    tr_data_loader = [\n",
    "        (sample, label, length, sent)\n",
    "        for sample, label, length, sent in zip(\n",
    "            tr_data_vecs, tr_labels, tr_lens, tr_sents\n",
    "        )\n",
    "    ]\n",
    "    val_data_loader = [\n",
    "        (sample, label, length, sent)\n",
    "        for sample, label, length, sent in zip(\n",
    "            val_data_vecs, val_labels, val_lens, val_sents\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return tr_data_loader, val_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZvbpqCwXY3E"
   },
   "outputs": [],
   "source": [
    "tr_data_loader, val_data_loader = prepare_dataloader_with_padding(\n",
    "    tr_data, val_data, word_to_ix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBSJgL9rXgDG"
   },
   "outputs": [],
   "source": [
    "def create_dataloader_iterators_with_padding(\n",
    "    tr_data_loader, val_data_loader, BATCH_SIZE\n",
    "):\n",
    "    train_iterator = DataLoader(\n",
    "        tr_data_loader,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    valid_iterator = DataLoader(\n",
    "        val_data_loader,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_iterator, valid_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XLM8WHqZCim"
   },
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator = create_dataloader_iterators_with_padding(\n",
    "    tr_data_loader, val_data_loader, BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrBaXBnaWk9t",
    "outputId": "f0ab7a27-1690-4893-b2c2-62695c6776ff"
   },
   "outputs": [],
   "source": [
    "next(iter(train_iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![embedding](https://github.com/bentrevett/pytorch-sentiment-analysis/raw/b4efbefa47672174394a8b6a27d4e7bc193bc224/assets/sentiment8.png)\n",
    "\n",
    "_image from bentrevett_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k85Rbx-3ZLq9"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BoWClassifierWithEmbedding(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size, embedding_dim):\n",
    "        super(BoWClassifierWithEmbedding, self).__init__()\n",
    "\n",
    "        # We define the embedding layer here\n",
    "        # It will convert a list of ids: [1, 50, 64, 2006]\n",
    "        # Into a list of vectors, one for each word\n",
    "        # The embedding layer will learn the vectors from the contexts\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=3001)\n",
    "        # We could also load precomputed embeddings, e.g. GloVe, in some cases we don't want to train the embedding layer\n",
    "        # In this case we enable the training\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "        self.linear = nn.Linear(embedding_dim, num_labels)\n",
    "\n",
    "    def forward(self, text, sequence_lens):\n",
    "        # First we create the embedded vectors\n",
    "        embedded = self.embedding(text)\n",
    "        # We need a pooling to convert a list of embedded words to a sentence vector\n",
    "        # We could have chosen different pooling, e.g. min, max, average..\n",
    "        # With LSTM we also do a pooling, just smarter\n",
    "        pooled = F.max_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)\n",
    "        return F.log_softmax(self.linear(pooled), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of the LSTM layer..\n",
    "\n",
    "![lstm](https://i.stack.imgur.com/SjnTl.png)\n",
    "\n",
    "_image from stackoverflow_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=3001)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        # Documentation: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "        # Dropout to overcome overfitting\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, text, sequence_lens):\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # To ensure LSTM doesn't learn gradients for the id of the padding symbol\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, sequence_lens, enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "        packed_outputs, (h, c) = self.lstm(packed)\n",
    "        # extract LSTM outputs (not used here)\n",
    "        lstm_outputs, lens = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outputs, batch_first=True\n",
    "        )\n",
    "\n",
    "        # We use the last hidden vector from LSTM\n",
    "        y = self.linear(h[-1])\n",
    "        log_probs = F.log_softmax(y, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QgKkr4bZSPG"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = VOCAB_SIZE + 2\n",
    "OUTPUT_DIM = 2\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 20\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# model = BoWClassifierWithEmbedding(OUTPUT_DIM, INPUT_DIM, EMBEDDING_DIM)\n",
    "model = LSTMClassifier(OUTPUT_DIM, INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVrF6CgmZUeg"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "bHCHriijZU9F",
    "outputId": "75570e6e-4069-48bd-d20d-d2c4529b5a09"
   },
   "outputs": [],
   "source": [
    "training_loop(epoch_number=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLx6p6f2Y69C"
   },
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To completely understand the transformers architecture look at this lecture held by Judit Acs (on the course of Introduction to Python and Natural Language Technologies in BME): \n",
    "- https://github.com/bmeaut/python_nlp_2021_spring/blob/main/lectures/09_Transformers_BERT.ipynb\n",
    "\n",
    "Here I will only include and present the necessary details _from the lecture_ about transformers and BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with recurrent neural networks:\n",
    "\n",
    "Recall that we used recurrent neural cells, specifically LSTMs to encode a list of vectors into a sentence vector.\n",
    "\n",
    "- Problem 1. No parallelism\n",
    "\n",
    "        - LSTMs are recurrent, they rely on their left and right history, so the symbols need to be processed in order -> no parallelism.\n",
    "\n",
    "- Problem 2. Long-range dependencies\n",
    "\n",
    "        - Long-range dependencies are not infrequent in NLP.\n",
    "\n",
    "        - \"The people/person who called and wanted to rent your house when you go away next year are/is from California\" -- Miller & Chomsky 1963\n",
    "\n",
    "        - LSTMs have a problem capturing these because there are too many backpropagation steps between the symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduced in [Attention Is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) by Vaswani et al., 2017\n",
    "\n",
    "Transformers solve Problem 1 by relying purely on attention instead of recurrence.\n",
    "\n",
    "Not having recurrent connections means that sequence position no longer matters.\n",
    "\n",
    "Recurrence is replaced by self attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformers are available in the __transformers__ Python package: https://github.com/huggingface/transformers.\n",
    "- There are thousands of pretrained transformers models in different languages and with different architectures. \n",
    "- With the huggingface package there is a unified interface to download and use all the models. Browse https://huggingface.co/models for more!\n",
    "- There is also a great blog post to understand the architecture of transformers: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "[BERT](https://www.aclweb.org/anthology/N19-1423/): Pre-training of Deep Bidirectional Transformers for Language Understanding by Devlin et al. 2018, 17500 citations\n",
    "\n",
    "[BERTology](https://huggingface.co/transformers/bertology.html) is the nickname for the growing amount of BERT-related research.\n",
    "\n",
    "BERT trains a transformer model on two tasks:\n",
    "\n",
    "- Masked language model:\n",
    "\n",
    "    - 15% of the tokenswordpieces are selected at the beginning.\n",
    "    - 80% of those are replaced with [MASK],\n",
    "    - 10% are replaced with a random token,\n",
    "    - 10% are kept intact.\n",
    "\n",
    "- Next sentence prediction:\n",
    "    - Are sentences A and B consecutive sentences?\n",
    "    - Generate 50-50%.\n",
    "    - Binary classification task.\n",
    "    \n",
    "\n",
    "### Training, Finetuning BERT\n",
    "\n",
    "- BERT models are (masked-)language models that were usually trained on large corporas.\n",
    "\n",
    "- e.g. BERT base model was trained on BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia.\n",
    "\n",
    "#### Finetuning\n",
    "\n",
    "- Get a trained BERT model.\n",
    "- Add a small classification layer on top (typically a 2-layer MLP).\n",
    "- Train BERT along with the classification layer on an annotated dataset.\n",
    "- Much smaller than the data BERT was trained on\n",
    "- Another option: freeze BERT and train the classification layer only.\n",
    "    - Easier training regime.\n",
    "    - Smaller memory footprint.\n",
    "    - Worse performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg\" alt=\"finetune\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece tokenizer\n",
    "- BERT has its own tokenizer\n",
    "- All inputs must be tokenized with BERT \n",
    "- You don't need to remove stopwords, lemmatize, preprocess the input for BERT\n",
    "\n",
    "- It is a middle ground between word and character tokenization.\n",
    "\n",
    "- Static vocabulary:\n",
    "    - Special tokens: [CLS], [SEP], [MASK], [UNK]\n",
    "    - It tokenizes everything, falling back to characters and [UNK] if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkBZHPfqPwZ-"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467,
     "referenced_widgets": [
      "096810167d924a7da4ad7c4538c50a04",
      "0af1cd00129d49d4a31217aa51dc6492",
      "ae22a36a43f9401d857fc676be1278f5",
      "07db6b1c39654a22900ed9b917e34d25",
      "a7010a8925694b04821fe56296bfbf2f",
      "dfe4c176f62f4d72a3eed57e1b1fc87f",
      "c9ca64235fde44c89bb911d352610442",
      "36726f26d1f143fd9474d5dce2619792",
      "8d4f4440a6a04ed6bc5a8eb4db590298",
      "24819d782a6140608a6c588afbc9b702",
      "1d6df86820ec45d6994ac74159ee9228",
      "2bb76615fa704275a081f03ea3f8cf5e",
      "a333b5da45ef4fb1932632601ad4f6ad",
      "206f338165c1467ea0bcf55d9d471af7",
      "5ae77de2d5d94c2788ea9080584d713d",
      "580b5a5e7090490db50460666bac7c81",
      "0b282fb195f24780a0519b91c4f1b344",
      "99d43a7fb97c4ada9ea664b2b171f732",
      "55e8e84304c74f4797c783a335a0f3e3",
      "8c607ade94ff4d6eb5a9db3609d83f92",
      "b38a23c10fbd4718b4b035969b1c5e63",
      "5f0d6531228a4157bddadc813d5534eb",
      "d0cf263890ab4d12a1c4926081494f7a",
      "9b5d87c13f3b41d0ab97b373ef386bdb",
      "d99fe5b061ea487db430d81effd5d26d",
      "e2d0984c7c1d4b0b94730d79d6af05fb",
      "a1e5eb0cf27e496ba2bca21d2c58f110",
      "53bab86ab477490091c0b93987200f7b",
      "0cca3ccf52bc49d9852f05004ed055da",
      "235d878939c344ab875231cb878fca53",
      "5feb0980894e4dd9a7598a1f25b54a0c",
      "240cae4992ce4038a6e919cad26ed18c",
      "74f840feaa5e496495f904d01df4fb07",
      "3c1ebe4b499847c39be7e3c3d9e8633f",
      "7bdcda40469641c6bc342ee12be14cc9",
      "2c26c0d144f240569046450fbec8f22c",
      "77892da04de543e9874c6895d67c2b63",
      "1edb014f1bd2456ea3e32c3d88fd978d",
      "023713fa09c24b2f8b3af722b25c79fa",
      "e6f37ec804244e499af12cedc804e130",
      "d60c5031673c41de90ab6f9db55641c2",
      "dde18a6e6bd74e88942a953792f4f37d",
      "94583cf009f54a4cba00bb5893e2d2c3",
      "c98ae3150e2747fea3027f7a79deb4be",
      "2fb629ac07334b768b12060e75972976",
      "df7172cfa535480690a6c9a402460f4e",
      "5b361c6a04024f92a8f6b5d39c921826",
      "22c034898e3f4bffa2e2b3e72dfd5125",
      "fc8e95563eef4cd3bd2741cf9545d1a7",
      "e5bdf71518aa4098885d923927eeb1d8",
      "e17dc125b2414d09b29e5ca69a6f9231",
      "7345e744a8d1426fb5cab6f8cd325a90",
      "aa7b97e741394bc08e108d2187750cb3",
      "b60f66d91f4a4b409a69f37c40c70dd1",
      "2e0991876c794ff9bc1550f5dcabac73"
     ]
    },
    "id": "__eq23ZTP21d",
    "outputId": "76632030-18a2-4ef7-98ed-ca336363e0cf"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tokenizer))\n",
    "print(len(tokenizer.get_vocab()))\n",
    "\n",
    "tokenizer.tokenize(\"My shihtzu's name is Maszat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"There are black cats and black dogs.\", \"Another sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a BertForSequenceClassification model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BertForSequenceClassification__ is a helper class to train transformer-based BERT models. It puts a classification layer on top of a pretrained model.\n",
    "\n",
    "Read more in the documentation: https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOOymZT0ZNs9"
   },
   "outputs": [],
   "source": [
    "# We only want to finetune the classification layer on top of BERT\n",
    "for p in model.base_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print(f\"The BERT model has {len(params)} different named parameters.\")\n",
    "\n",
    "print(\"==== Embedding Layer ====\\n\")\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(f\"{p[0]} {str(tuple(p[1].size()))}\")\n",
    "\n",
    "print(\"\\n==== First Transformer ====\\n\")\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(f\"{p[0]} {str(tuple(p[1].size()))}\")\n",
    "\n",
    "print(\"\\n==== Output Layer ====\\n\")\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(f\"{p[0]} {str(tuple(p[1].size()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data_loader, val_data_loader = prepare_dataloader_with_padding(\n",
    "    tr_data, val_data, word_to_ix\n",
    ")\n",
    "\n",
    "train_iterator, valid_iterator = create_dataloader_iterators_with_padding(\n",
    "    tr_data_loader, val_data_loader, BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_prec = 0\n",
    "    train_epoch_recall = 0\n",
    "    train_epoch_fscore = 0\n",
    "    model.train()\n",
    "\n",
    "    # We use our own iterator but now use the raw texts instead of the ID tokens\n",
    "    for train_batch in train_iterator:\n",
    "        labels = train_batch[1]\n",
    "        texts = train_batch[3]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # We use BERT's own tokenizer on raw texts\n",
    "        # Check the documentation: https://huggingface.co/transformers/main_classes/tokenizer.html\n",
    "        encoded = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # BERT converts texts into IDs of its own vocabulary\n",
    "        input_ids = encoded[\"input_ids\"].to(device)\n",
    "        # Mask to avoid performing attention on padding token indices.\n",
    "        attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Run the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        predictions = outputs[1]\n",
    "        prec, recall, fscore = calculate_performance(predictions, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += loss.item()\n",
    "        train_epoch_prec += prec.item()\n",
    "        train_epoch_recall += recall.item()\n",
    "        train_epoch_fscore += fscore.item()\n",
    "\n",
    "    train_loss = train_epoch_loss / len(train_iterator)\n",
    "    train_prec = train_epoch_prec / len(train_iterator)\n",
    "    train_rec = train_epoch_recall / len(train_iterator)\n",
    "    train_fscore = train_epoch_fscore / len(train_iterator)\n",
    "\n",
    "    # And validate your model on the validation set\n",
    "    valid_epoch_loss = 0\n",
    "    valid_epoch_prec = 0\n",
    "    valid_epoch_recall = 0\n",
    "    valid_epoch_fscore = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for valid_batch in valid_iterator:\n",
    "            labels = valid_batch[1]\n",
    "            texts = valid_batch[3]\n",
    "\n",
    "            encoded = tokenizer(\n",
    "                texts,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids = encoded[\"input_ids\"].to(device)\n",
    "            attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            predictions = outputs[1]\n",
    "            prec, recall, fscore = calculate_performance(predictions, labels)\n",
    "\n",
    "            # We add batch-wise loss to the epoch-wise loss\n",
    "            valid_epoch_loss += loss.item()\n",
    "            valid_epoch_prec += prec.item()\n",
    "            valid_epoch_recall += recall.item()\n",
    "            valid_epoch_fscore += fscore.item()\n",
    "\n",
    "    valid_loss = valid_epoch_loss / len(valid_iterator)\n",
    "    valid_prec = valid_epoch_prec / len(valid_iterator)\n",
    "    valid_rec = valid_epoch_recall / len(valid_iterator)\n",
    "    valid_fscore = valid_epoch_fscore / len(valid_iterator)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(\n",
    "        f\"\\tTrain Loss: {train_loss:.3f} | Train Prec: {train_prec*100:.2f}% | Train Rec: {train_rec*100:.2f}% | Train Fscore: {train_fscore*100:.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\t Val. Loss: {valid_loss:.3f} |  Val Prec: {valid_prec*100:.2f}% | Val Rec: {valid_rec*100:.2f}% | Val Fscore: {valid_fscore*100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Deep learning - practical lesson",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "dlpr",
   "language": "python",
   "name": "dlpr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "023713fa09c24b2f8b3af722b25c79fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "07db6b1c39654a22900ed9b917e34d25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d4f4440a6a04ed6bc5a8eb4db590298",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36726f26d1f143fd9474d5dce2619792",
      "value": 231508
     }
    },
    "096810167d924a7da4ad7c4538c50a04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae22a36a43f9401d857fc676be1278f5",
       "IPY_MODEL_07db6b1c39654a22900ed9b917e34d25",
       "IPY_MODEL_a7010a8925694b04821fe56296bfbf2f"
      ],
      "layout": "IPY_MODEL_0af1cd00129d49d4a31217aa51dc6492"
     }
    },
    "0af1cd00129d49d4a31217aa51dc6492": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b282fb195f24780a0519b91c4f1b344": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0cca3ccf52bc49d9852f05004ed055da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d6df86820ec45d6994ac74159ee9228": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1edb014f1bd2456ea3e32c3d88fd978d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c98ae3150e2747fea3027f7a79deb4be",
      "placeholder": "​",
      "style": "IPY_MODEL_94583cf009f54a4cba00bb5893e2d2c3",
      "value": " 570/570 [00:00&lt;00:00, 13.2kB/s]"
     }
    },
    "206f338165c1467ea0bcf55d9d471af7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99d43a7fb97c4ada9ea664b2b171f732",
      "placeholder": "​",
      "style": "IPY_MODEL_0b282fb195f24780a0519b91c4f1b344",
      "value": "Downloading: 100%"
     }
    },
    "22c034898e3f4bffa2e2b3e72dfd5125": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa7b97e741394bc08e108d2187750cb3",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7345e744a8d1426fb5cab6f8cd325a90",
      "value": 440473133
     }
    },
    "235d878939c344ab875231cb878fca53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "240cae4992ce4038a6e919cad26ed18c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24819d782a6140608a6c588afbc9b702": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2bb76615fa704275a081f03ea3f8cf5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_206f338165c1467ea0bcf55d9d471af7",
       "IPY_MODEL_5ae77de2d5d94c2788ea9080584d713d",
       "IPY_MODEL_580b5a5e7090490db50460666bac7c81"
      ],
      "layout": "IPY_MODEL_a333b5da45ef4fb1932632601ad4f6ad"
     }
    },
    "2c26c0d144f240569046450fbec8f22c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6f37ec804244e499af12cedc804e130",
      "placeholder": "​",
      "style": "IPY_MODEL_023713fa09c24b2f8b3af722b25c79fa",
      "value": "Downloading: 100%"
     }
    },
    "2e0991876c794ff9bc1550f5dcabac73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fb629ac07334b768b12060e75972976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b361c6a04024f92a8f6b5d39c921826",
       "IPY_MODEL_22c034898e3f4bffa2e2b3e72dfd5125",
       "IPY_MODEL_fc8e95563eef4cd3bd2741cf9545d1a7"
      ],
      "layout": "IPY_MODEL_df7172cfa535480690a6c9a402460f4e"
     }
    },
    "36726f26d1f143fd9474d5dce2619792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c1ebe4b499847c39be7e3c3d9e8633f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c26c0d144f240569046450fbec8f22c",
       "IPY_MODEL_77892da04de543e9874c6895d67c2b63",
       "IPY_MODEL_1edb014f1bd2456ea3e32c3d88fd978d"
      ],
      "layout": "IPY_MODEL_7bdcda40469641c6bc342ee12be14cc9"
     }
    },
    "53bab86ab477490091c0b93987200f7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55e8e84304c74f4797c783a335a0f3e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "580b5a5e7090490db50460666bac7c81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f0d6531228a4157bddadc813d5534eb",
      "placeholder": "​",
      "style": "IPY_MODEL_b38a23c10fbd4718b4b035969b1c5e63",
      "value": " 28.0/28.0 [00:00&lt;00:00, 581B/s]"
     }
    },
    "5ae77de2d5d94c2788ea9080584d713d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c607ade94ff4d6eb5a9db3609d83f92",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55e8e84304c74f4797c783a335a0f3e3",
      "value": 28
     }
    },
    "5b361c6a04024f92a8f6b5d39c921826": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e17dc125b2414d09b29e5ca69a6f9231",
      "placeholder": "​",
      "style": "IPY_MODEL_e5bdf71518aa4098885d923927eeb1d8",
      "value": "Downloading: 100%"
     }
    },
    "5f0d6531228a4157bddadc813d5534eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5feb0980894e4dd9a7598a1f25b54a0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7345e744a8d1426fb5cab6f8cd325a90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "74f840feaa5e496495f904d01df4fb07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77892da04de543e9874c6895d67c2b63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dde18a6e6bd74e88942a953792f4f37d",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d60c5031673c41de90ab6f9db55641c2",
      "value": 570
     }
    },
    "7bdcda40469641c6bc342ee12be14cc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c607ade94ff4d6eb5a9db3609d83f92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d4f4440a6a04ed6bc5a8eb4db590298": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94583cf009f54a4cba00bb5893e2d2c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99d43a7fb97c4ada9ea664b2b171f732": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b5d87c13f3b41d0ab97b373ef386bdb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1e5eb0cf27e496ba2bca21d2c58f110": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74f840feaa5e496495f904d01df4fb07",
      "placeholder": "​",
      "style": "IPY_MODEL_240cae4992ce4038a6e919cad26ed18c",
      "value": " 455k/455k [00:00&lt;00:00, 1.93MB/s]"
     }
    },
    "a333b5da45ef4fb1932632601ad4f6ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7010a8925694b04821fe56296bfbf2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d6df86820ec45d6994ac74159ee9228",
      "placeholder": "​",
      "style": "IPY_MODEL_24819d782a6140608a6c588afbc9b702",
      "value": " 226k/226k [00:00&lt;00:00, 814kB/s]"
     }
    },
    "aa7b97e741394bc08e108d2187750cb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae22a36a43f9401d857fc676be1278f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9ca64235fde44c89bb911d352610442",
      "placeholder": "​",
      "style": "IPY_MODEL_dfe4c176f62f4d72a3eed57e1b1fc87f",
      "value": "Downloading: 100%"
     }
    },
    "b38a23c10fbd4718b4b035969b1c5e63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b60f66d91f4a4b409a69f37c40c70dd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c98ae3150e2747fea3027f7a79deb4be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9ca64235fde44c89bb911d352610442": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0cf263890ab4d12a1c4926081494f7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d99fe5b061ea487db430d81effd5d26d",
       "IPY_MODEL_e2d0984c7c1d4b0b94730d79d6af05fb",
       "IPY_MODEL_a1e5eb0cf27e496ba2bca21d2c58f110"
      ],
      "layout": "IPY_MODEL_9b5d87c13f3b41d0ab97b373ef386bdb"
     }
    },
    "d60c5031673c41de90ab6f9db55641c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d99fe5b061ea487db430d81effd5d26d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cca3ccf52bc49d9852f05004ed055da",
      "placeholder": "​",
      "style": "IPY_MODEL_53bab86ab477490091c0b93987200f7b",
      "value": "Downloading: 100%"
     }
    },
    "dde18a6e6bd74e88942a953792f4f37d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df7172cfa535480690a6c9a402460f4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfe4c176f62f4d72a3eed57e1b1fc87f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e17dc125b2414d09b29e5ca69a6f9231": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2d0984c7c1d4b0b94730d79d6af05fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5feb0980894e4dd9a7598a1f25b54a0c",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_235d878939c344ab875231cb878fca53",
      "value": 466062
     }
    },
    "e5bdf71518aa4098885d923927eeb1d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6f37ec804244e499af12cedc804e130": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc8e95563eef4cd3bd2741cf9545d1a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e0991876c794ff9bc1550f5dcabac73",
      "placeholder": "​",
      "style": "IPY_MODEL_b60f66d91f4a4b409a69f37c40c70dd1",
      "value": " 420M/420M [00:16&lt;00:00, 25.9MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
