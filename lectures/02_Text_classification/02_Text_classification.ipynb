{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "### Natural Language Processing and Information Extraction,  2022 WS\n",
    "Lecture 2, 10/21/2021\n",
    "\n",
    "GÃ¡bor Recski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant chapters\n",
    "SLP 4.1, 4.2, 4.3, 4.7, 4.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "from math import exp, log\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification tasks are some of the simplest and most common NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks may involve classifying **documents**, **sentences** or **words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sa](media/sa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spam](media/spam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-/utterance-level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intent detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![intent](media/intent.png)\n",
    "\n",
    "([SentiOne](https://sentione.com/blog/new-state-of-the-art-intent-detection-model-from-sentione))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slot-filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![slot](media/slot.png)\n",
    "\n",
    "([Hung-yi Lee's Slides](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/RNN%20(v2).pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic representation of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bow](media/bow.png)\n",
    "\n",
    "([SLP Ch.4](https://web.stanford.edu/~jurafsky/slp3/4.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the bag-of-words representation for learning to classify documents.\n",
    "Each document in a training dataset will be represented by its words: $d_i = (x_1, x_2, \\ldots, x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a document $d$ and class $c$, maximize the likelihood that $d$ belongs to $c$ over all classes $c\\in C$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{c\\in C}{\\operatorname{argmax}}P(c|d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply [Bayes' rule](https://en.wikipedia.org/wiki/Bayes%27_theorem):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(c|d) = \\frac{P(d|c)P(c)}{P(d)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When classifying a document, the denominator is the same for all classes and can be dropped:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{c\\in C}{\\operatorname{argmax}}P(d|c)P(c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a document is represented by features $x_1, x_2, \\ldots x_n$ then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(d|c) = P(x_1, x_2, \\ldots, x_n|c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make an **assumption** that the features are independent, then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_1, x_2, \\ldots, x_n|c) = P(x_1|c)P(x_2|c)\\ldots P(x_n|c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the bag-of-words model, the features are the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c_{\\text{NB}} = \\underset{c_j\\in C}{\\operatorname{argmax}}P(c_j)\\prod_{w_i \\in D}{P(w_i|c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid underflow errors, we usually implement this in log-space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c_{\\text{NB}} = \\underset{c_j\\in C}{\\operatorname{argmax}}\\log P(c_j)+\\sum_{w_i \\in D}{\\log P(w_i|c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And instead of multiplying very small numbers we are now adding up weights. The class with the highest score is still the most likely class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we estimate (learn) the paramteres $P(w_i|c_j)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way is by counting occurrences in the training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_i|c_j) = \\frac{\\text{count}(w_i, c_j)}{\\sum_{w\\in V}{\\text{count}(w, c_j)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $V$ is the **vocabulary** of all words in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word is not in the vocabulary (out-of-vocabulary, OOV), we don't want to introduce zero probabilities. The simplest solution is Laplace (add-1) **smoothing**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_i|c_j) = \\frac{\\text{count}(w_i, c_j)+1}{\\sum_{w\\in V}{(\\text{count}(w, c_j)+1)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another simple solution is to leave OOVs out of the equation completely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation of a Naive Bayes classifier with this simplest approach will follow later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes may be naive, but it is powerful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- very fast, with low memory requirements\n",
    "- robust to irrelevant features\n",
    "- works well in domains with many equally important features\n",
    "- a strong, dependable baseline for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bag-of-words may be naive, but it is powerful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- efficient: vocabularies are large but number of relevant dimensions is limited\n",
    "- interpretable: lets us understand what a model learned\n",
    "- robust, low-resource, does not depend on genre, domain, language-specific resources, etc.\n",
    "\n",
    "**In natural language, most of the information comes from words!**\n",
    "\n",
    "(The word entropy of natural language is 12-16 bits ([Kornai 2008](https://eprints.sztaki.hu/7913/1/Kornai_1762289_ny.pdf) Section 7.1), punctuation doesn't contribute much (7% for English, see [Brown 1992](https://aclanthology.org/J92-1002.pdf)), and syntax cannot contribute more than 2 bits per word ($C_n\\sim 4^n$, where $C_n$ are the [Catalan numbers](https://en.wikipedia.org/wiki/Catalan_number)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eval](media/eval.png)\n",
    "\n",
    "([SLP Ch.4](https://web.stanford.edu/~jurafsky/slp3/4.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F-measure** is the (weighted) harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_\\beta = \\frac{(\\beta^2+1)PR}{\\beta^2P+R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced F-measure ($\\beta=1$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_1 = \\frac{2PR}{P+R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 is used everywhere in **academia** (challenges, leaderboards), but precision and recall are usually **not equally important in real-world applications**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small differences in F-score may not mean small differences in output. You should **always look at precision and recall separately**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eval2](media/eval2.png)\n",
    "\n",
    "([SLP Ch.4](https://web.stanford.edu/~jurafsky/slp3/4.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro- vs. micro-averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eval3](media/eval3.png)\n",
    "\n",
    "([SLP Ch.4](https://web.stanford.edu/~jurafsky/slp3/4.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a BOW classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNBClassifier():\n",
    "    def __init__(self):\n",
    "        self.word_count = Counter()\n",
    "        self.count_by_class = defaultdict(Counter)\n",
    "        self.labels = set()\n",
    "    \n",
    "    def count_words(self, docs):\n",
    "        for words, label in docs:\n",
    "            self.labels.add(label)\n",
    "            for word in set(words):\n",
    "                self.word_count[word] += 1\n",
    "                self.count_by_class[label][word] += 1\n",
    "    \n",
    "    def calculate_weights(self):\n",
    "        self.weights = {\n",
    "            word: {\n",
    "                label: log((self.count_by_class[label][word] + 1) / (count + len(self.word_count)))\n",
    "                for label in self.labels}\n",
    "            for word, count in self.word_count.items()}\n",
    "        \"\"\"\n",
    "        self.weights = {}\n",
    "        for word, count in self.word_count.items():\n",
    "            self.weights[word] = {}\n",
    "            for label in self.labels:\n",
    "                prob = (self.count_by_class[label][word] + 1) / (count + len(self.word_count))  # add-1 smoothing\n",
    "                self.weights[word][label] = log(prob)\n",
    "        \"\"\"\n",
    "        \n",
    "    def get_doc_weights(self, doc):\n",
    "        return {\n",
    "            label: sum(\n",
    "                self.weights[word][label] if word in self.weights else log(1)\n",
    "                for word in doc)\n",
    "            for label in self.labels}\n",
    "    \n",
    "    def predict_label(self, doc):\n",
    "        doc_weights = self.get_doc_weights(doc)\n",
    "        return sorted(doc_weights.items(), key=lambda x: -x[1])[0][0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** this implementation is not very efficient, normally you would use an ML library with efficient data structures and algorithms. E.g. here you could have used [scikit-learn's NB implementation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = SimpleNBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = [\n",
    "    ((\"He\", \"is\", \"nice\"), \"POS\"),\n",
    "    ((\"He\", \"is\", \"stupid\"), \"NEG\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.count_words(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.calculate_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = ('This', 'movie', 'is', 'stupid')\n",
    "doc2 = ('This', 'movie', 'is', 'nice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.get_doc_weights(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict_label(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.get_doc_weights(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict_label(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a real dataset: [50k movie reviews from IMDB](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "with open('data/imdb_dataset.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for text, label in tqdm(reader):\n",
    "        words = nltk.word_tokenize(text)        \n",
    "        docs.append((words, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = SimpleNBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.count_words(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.calculate_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = ('This', 'movie', 'is', 'stupid')\n",
    "doc2 = ('I', 'loved', 'this', 'movie')\n",
    "doc3 = ('This', 'is', 'the', 'most', 'famous', 'movie', 'by', 'Aaron', 'Sorkin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict_label(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict_label(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict_label(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.get_doc_weights(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.get_doc_weights(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.get_doc_weights(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc1:\n",
    "    print(word, nb.weights[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc2:\n",
    "    print(word, nb.weights[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc3:\n",
    "    print(word, nb.weights[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(('{:>12}'*4).format('word', 'pos', 'neg', 'diff'))\n",
    "print('='*48)\n",
    "for word in doc3:\n",
    "    print('{:>12}{:>12.2f}{:>12.2f}{:>12.2f}'.format(word, nb.weights[word]['positive'], nb.weights[word]['negative'], nb.weights[word]['positive'] - nb.weights[word]['negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This inspection of feature weights is how you might be able to \"debug\" a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train and evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, dev_docs, test_docs = docs[:40000], docs[40001:45000], docs[45001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = SimpleNBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.count_words(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.calculate_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "for doc, label in dev_docs:\n",
    "    pred_label = nb.predict_label(doc)\n",
    "    if pred_label == 'positive':\n",
    "        if label == 'positive':\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if label == 'positive':\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = (2*precision*recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precision: {precision:5.2f}, Recall: {recall:5.2f}, F1: {f1:5.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLP 3.1, 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ngrams are the sequences of words in a text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is the most famous movie by Aaron Sorkin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = text.split()\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [unigrams[i:i+2] for i in range(len(unigrams) - 1)]\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [unigrams[i:i+3] for i in range(len(unigrams) - 2)]\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-ngrams are a natural extension of bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of ngrams is also characteristic of a language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngram frequencies in a language can be used to approximate the __probability of any text__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as __ngram language modeling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why model the probability of strings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- autocomplete, autocorrect\n",
    "- speech recognition\n",
    "- machine translation\n",
    "- augmentative and alternative communication (AAC)\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the probability of an upcoming word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_i | w_1, w_2, \\ldots, w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ngram language model __approximates__ this with the last $n-1$ words, i.e. by looking at a window of $n$ words only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_i | w_{i-2}, w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be approximated based on counts of trigrams and bigrams in a corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_i | w_{i-2}, w_{i-1}) \\sim \\frac{c(w_{i-2}, w_{i-1}, w_{i})}{c(w_{i-2}, w_{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The larger the value of $n$, the higher the number of parameters, and even very large corpora become small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "with open('data/shakespeare.txt') as f:\n",
    "    for line in tqdm(f):\n",
    "        words = tuple(['<s>'] + [w.lower() for w in nltk.word_tokenize(line)] + ['</s>'])\n",
    "        for i in range(len(words) - 2):\n",
    "            w1, w2, w3 = words[i:i+3]\n",
    "            trigrams[w1][w2][w3] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_word(words):\n",
    "    assert len(words) >= 2\n",
    "    w1, w2 = words[-2].lower(), words[-1].lower()\n",
    "    words, freqs = zip(*trigrams[w1][w2].items())\n",
    "    p = np.array(freqs) / sum(freqs)\n",
    "    return np.random.choice(words, p=p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next_word(('<s>', 'I'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(first_word):\n",
    "    text = ['<s>', first_word.lower()]\n",
    "    while text[-1] != '</s>':\n",
    "        text.append(get_next_word(text))\n",
    "    return ' '.join(text[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentence('She')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_\"If oneâs view of language is that it is a probability\n",
    "distribution over strings of letter or sounds, one turns oneâs back on the scientific\n",
    "achievements of the ages and foreswears the opportunity that computers offer to carry\n",
    "that enterprise forward.\"_ [(Kay 2006, p.430)](https://aclanthology.org/J05-4001.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
